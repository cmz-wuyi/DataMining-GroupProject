# Data Center Energy Consumption and Cooling Optimization: A Data Mining Project

This is a group project for the **CDS 535 Data Mining** course.

## 1. Project Overview

This project analyzes the operational data from a modern data center's cooling system using a suite of data mining techniques. The core objective is to understand the complex relationships between server workload, environmental parameters, and energy consumption (chillers, AHUs).

By following the **CRISP-DM (Cross-Industry Standard Process for Data Mining)** framework, we aim to identify potential energy-saving opportunities while ensuring the data center's temperature stability.

## 2. Research Objectives

The project is structured around four primary data mining tasks:

1.  **Prediction (Regression):** To forecast cooling unit power consumption (kW) and temperature deviation (Â°C).
2.  **Classification:** To classify the data center's operational state (e.g., 'High', 'Medium', 'Low') based on a custom-defined "Energy Efficiency" metric.
3.  **Clustering:** To automatically discover and identify distinct "cooling operational modes" from the data.
4.  **Association Rules:** To find strong "if-then" rules connecting environmental parameters (like ambient temperature) with energy consumption patterns.

## 3. Datasets

This repository contains three versions of the dataset:

* **`dataset_no_units.csv`**: The raw, original dataset. It includes 3,498 hourly time-series records with features like server workload, inlet/outlet temperatures, ambient temperature, power consumption, and chiller/AHU usage.
* **`dataset_preprocessed.csv`**: A preprocessed dataset used for prediction, classification, and clustering. This file was generated by adding an `Energy_Efficiency` feature (calculated as `Workload / Power`) and its corresponding discretized label (`Energy_Efficiency_Class`).
* **`dataset_for_association.csv`**: A preprocessed dataset used specifically for association rule mining. In this file, key numerical features (like `Ambient_Temperature`, `Server_Workload`, and `Cooling_Unit_Power_Consumption`) have been discretized into categorical bins (e.g., 'Low', 'Medium', 'High').

## 4. How to Run the Analysis

The complete analysis is contained within the `Test.ipynb` Jupyter Notebook.

1.  **Clone the Repository:**
    ```bash
    git clone 
    ```

2.  **Install Dependencies:**
    Ensure your Python environment has the necessary libraries installed.
    ```bash
    pip install pandas numpy matplotlib seaborn scikit-learn mlxtend jupyter
    ```

3.  **Run the Notebook:**
    Launch Jupyter Notebook and open `Test.ipynb`.
    ```bash
    jupyter notebook Test.ipynb
    ```
    Execute the cells in sequential order to replicate the full analysis pipeline, from data loading to model evaluation.

## 5. File Structure & Key Results

* `Test.ipynb`: The main Jupyter Notebook containing all Python code and analysis for the four research objectives.
* `dataset_*.csv`: The three data files as described above.
* `classification_importance.png`: A bar plot showing the most important features for predicting energy efficiency (generated by the Random Forest Classifier).
* `clustering_elbow_plot.png`: A line chart illustrating the "Elbow Method," which was used to determine the optimal number of clusters (k=4).
* `clustering_profile_heatmap.png`: **(Key Finding)** A heatmap visualizing the "profiles" (average feature values) for each of the 4 discovered clusters, allowing us to name and interpret them.
* `clustering_pca_plot.png`: A 2D scatter plot (using PCA for dimensionality reduction) showing the separation of the 4 clusters.
* `decision_tree.png`: A visualization of the trained Decision Tree for the classification task.
